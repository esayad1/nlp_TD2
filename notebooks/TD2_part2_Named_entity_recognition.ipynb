{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7c0b40",
   "metadata": {},
   "source": [
    "# TD2 part 2: Named entity recognition\n",
    "\n",
    "Dans ce TD, nous allons prendre un datasets où les noms de personnes sont taggés.<br>\n",
    "Nous allons transformer ces données en tenseurs X, y et attention_mask.<br>\n",
    "Nous allons créer un modèle RNN pour prédire si un mot est un nom de personne.<br>\n",
    "Nous allons ensuite créer la loop avec l'optimizer pour apprendre le modèle.<br>\n",
    "Du modèle appris (prédisant sur les tokens), nous allons postprocess les prédictions pour avoir les prédictions sur les noms.\n",
    "\n",
    "Un fois que la loop est créée et que le modèle apprend, nous allons changer la structure du modèle:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n",
    "Lorsqu'on a un bon modèle de prédiction pour les noms de personnes, nous allons l'appliquer à notre projet fil rouge.\n",
    "Utilisez-le tel que. Quelle accuracy ?\n",
    "Ré-entrainez la (les) dernière(s) couche(s) du modèle sur notre jeu de données. A-t-il gagné en accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86402ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:57:31.641737100Z",
     "start_time": "2023-11-27T19:57:21.408564700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f552fb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Télécharger le dataset MultiNERD FR [ici](https://github.com/Babelscape/multinerd)<br>\n",
    "Mettez les données dans le dossier data/raw du projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157bc913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:57:53.960078500Z",
     "start_time": "2023-11-27T19:57:53.907065600Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_multinerd_person_words(filename=\"../data/raw/train_fr.tsv\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        tagged_words = [line.strip().split(\"\\t\") for line in f]\n",
    "        \n",
    "        # Joining words until we meet a dot\n",
    "        # Word's label is 1 if 'PER' is in its tag\n",
    "        sentences = []\n",
    "        sentence_labels = []\n",
    "    \n",
    "        this_word = []\n",
    "        this_labels = []\n",
    "        for tagged_word in tagged_words:\n",
    "            if len(tagged_word) < 3:\n",
    "                # not a tagged word\n",
    "                continue\n",
    "            word = tagged_word[1]\n",
    "            tag = tagged_word[2]\n",
    "        \n",
    "            if word == '.':\n",
    "                sentences.append(\" \".join(this_word))\n",
    "                sentence_labels.append(this_labels)\n",
    "            \n",
    "                this_word = []\n",
    "                this_labels = []\n",
    "            else:\n",
    "                this_word.append(word)\n",
    "                this_labels.append(1 * tag.endswith(\"PER\"))\n",
    "\n",
    "    return sentences, sentence_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcba104b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T20:37:49.427837Z",
     "start_time": "2023-11-27T20:37:34.960071100Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, labels = extract_multinerd_person_words(\"../data/raw/multinerd_train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "c’ est ainsi que des firmes comme DuPont , Dow Chemical , Monsanto , American Cyanamid lancèrent la production en masse d' engrais minéraux\n"
     ]
    }
   ],
   "source": [
    "print(labels[1])\n",
    "print(sentences[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T20:37:56.093598200Z",
     "start_time": "2023-11-27T20:37:56.070295900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "ff9b09cc",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "En utilisant le tokenizer d'HuggingFace \"camembert-base\":\n",
    "- Transformer les phrases en tokens\n",
    "- Obtenez des vecteur y qui ont le même nombre d'entrées qu'il y a de tokens dans la phrase\n",
    "- Ayez un tenseur \"attention_mask\" pour savoir sur quels tokens on cherche à predire le label\n",
    "- Transformez les tokens en token_ids (avec le tokenizer)\n",
    "Avec tout cela, vous pouvez former vos tenseurs X, Y et attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8ff1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T20:38:04.457394100Z",
     "start_time": "2023-11-27T20:38:01.923266Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a3802c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T20:39:06.596262600Z",
     "start_time": "2023-11-27T20:39:06.551360400Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    tokens_label = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for word, label in zip(words, labels):\n",
    "        this_tokens = tokenizer.tokenize(word)\n",
    "        tokens += this_tokens\n",
    "        \n",
    "        this_labels = [0] * len(this_tokens)\n",
    "        this_labels[0] = label        \n",
    "        tokens_label += this_labels\n",
    "        \n",
    "        this_attention_mask = [1] + [0] * (len(this_tokens) - 1)\n",
    "        attention_mask += this_attention_mask\n",
    "        \n",
    "    return tokens, tokens_label, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066d1710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T20:39:14.063144Z",
     "start_time": "2023-11-27T20:39:14.037492400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens, label, padding_masks = build_tokens_and_labels_and_attention_mask(tokenizer, sentences[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d73a2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T20:39:17.812017800Z",
     "start_time": "2023-11-27T20:39:17.788461500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Il', '▁est', '▁incarné', '▁par', '▁A', 'ustin', '▁S', 'to', 'well']\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[1, 1, 1, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(label)\n",
    "print(padding_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e62c9b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:12:51.647514Z",
     "start_time": "2023-11-27T21:11:29.135854300Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "attention_masks = []\n",
    "for sentence, label in zip(sentences, labels):\n",
    "    tokens, label, padding_masks = build_tokens_and_labels_and_attention_mask(tokenizer, sentence, label)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    X.append(token_ids)\n",
    "    Y.append(label)\n",
    "    attention_masks.append(padding_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 et 9 et 9\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(attention_masks[0])} et {len(X[0])} et {len(Y[0])}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T21:21:20.557487900Z",
     "start_time": "2023-11-27T21:21:20.528806800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "feb94a39",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Contruisez un modèle RNN comme dans la partie 1. Pour l'instant, il prendra comme arguments:\n",
    "- Vocab size: le nombre de différents tokens du tokenizer (52 000 pour camembert-base)\n",
    "- Embedding dim: la dimension de l'embedding des tokens (par défaut 50)\n",
    "- hidden_dim: la dimension de l'état récurrent de votre RNN (par défaut 20)\n",
    "- tagset_size: la nombre de classes possibles pour les prédictions (ici 2)\n",
    "\n",
    "Dans le forward, votre modèle enchaînera les couches suivantes:\n",
    "- Un embedding\n",
    "- Un RNN\n",
    "- Un ReLU\n",
    "- Une couche linéaire\n",
    "- Un softmax pour que la somme des prédictions pour une entrée soit égale à 1 (la prédiction pour un élément et sa probabilité d'être dans chaque classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86e661ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:21:42.268185Z",
     "start_time": "2023-11-27T21:21:42.252236800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim = 50, hidden_dim = 20, target_size = 2):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embd = self.embedding(x)\n",
    "        out, _ = self.rnn(embd)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear(out)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04004a",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Je fournis ici une fonction prenant un modèle, des tenseurs X, y, attention_mask.\n",
    "Pour chaque batch:\n",
    "- La loop utilise le modèle pour prédire sur x_batch\n",
    "- Avec attention_mask, elle identifie sur quels tokens les prédictions compte\n",
    "- Elle regarde la cross entropy entre y\\[attention_ids\\] et yhat\\[attention_ids\\]\n",
    "- Elle output un dictionnaire avec le model et la loss au fur et à mesure des itérations\n",
    "\n",
    "Entraînez le modèle avec vos données. <br>\n",
    "Plottez la loss history.<br>\n",
    "Itérez sur le modèle pour l'améliorer:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58b9833f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:22:35.408416400Z",
     "start_time": "2023-11-27T21:22:35.283752300Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=52000, embedding_dim=50, hidden_dim=20, target_size=2)\n",
    "def train_model(model, X, y, attention_masks, n_epochs=100, lr=0.05, batch_size=128):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    loss_history = []\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y, attention_masks)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (x_batch, y_batch, mask) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            ids = mask.reshape(-1)\n",
    "            yhat = model(x_batch).reshape((-1, 2))[ids]\n",
    "            this_y = y_batch.reshape(-1)[ids]\n",
    "            \n",
    "            loss = loss_function(yhat, this_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            loss_history.append(loss.clone().detach())\n",
    "        \n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Got loss at {epoch}\", np.mean(loss_history[-10:]))\n",
    "    \n",
    "    return {\"model\": model, \"loss_history\": loss_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cf63f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:38:28.913324800Z",
     "start_time": "2023-11-27T21:22:44.772889600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got loss at 0 0.79506063\n",
      "Got loss at 10 0.004845935\n",
      "Got loss at 20 2.0942482e-05\n",
      "Got loss at 30 0.012983801\n",
      "Got loss at 40 0.0050909407\n",
      "Got loss at 50 9.8497476e-05\n",
      "Got loss at 60 0.00018422399\n",
      "Got loss at 70 5.7559075e-05\n",
      "Got loss at 80 0.552932\n",
      "Got loss at 90 0.00044702404\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'model': RNNModel(\n   (embedding): Embedding(52000, 50)\n   (rnn): RNN(50, 20, batch_first=True)\n   (linear): Linear(in_features=20, out_features=2, bias=True)\n ),\n 'loss_history': [tensor(0.7551),\n  tensor(0.6089),\n  tensor(0.3157),\n  tensor(0.2880),\n  tensor(0.1850),\n  tensor(0.0604),\n  tensor(0.1116),\n  tensor(0.0070),\n  tensor(0.0237),\n  tensor(0.0094),\n  tensor(0.0045),\n  tensor(0.0035),\n  tensor(0.0011),\n  tensor(0.0009),\n  tensor(0.0071),\n  tensor(0.0004),\n  tensor(0.0005),\n  tensor(0.0010),\n  tensor(3.0840e-05),\n  tensor(0.0008),\n  tensor(0.0017),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(0.0025),\n  tensor(0.0004),\n  tensor(0.0004),\n  tensor(5.5162e-05),\n  tensor(7.6584e-05),\n  tensor(0.0002),\n  tensor(0.0003),\n  tensor(0.0001),\n  tensor(1.2169e-05),\n  tensor(4.1069e-05),\n  tensor(0.0003),\n  tensor(3.1058e-05),\n  tensor(2.1839e-06),\n  tensor(0.0002),\n  tensor(1.2456e-05),\n  tensor(1.1194e-05),\n  tensor(5.3687e-05),\n  tensor(2.3374e-05),\n  tensor(0.0002),\n  tensor(1.5688e-05),\n  tensor(0.0003),\n  tensor(0.0003),\n  tensor(0.0002),\n  tensor(1.5812e-06),\n  tensor(1.4001e-05),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(10.2441),\n  tensor(0.0003),\n  tensor(2.6640e-05),\n  tensor(0.0002),\n  tensor(0.0009),\n  tensor(0.0001),\n  tensor(0.0001),\n  tensor(0.0006),\n  tensor(0.7186),\n  tensor(0.0006),\n  tensor(0.0428),\n  tensor(0.0062),\n  tensor(0.0005),\n  tensor(0.0172),\n  tensor(0.0124),\n  tensor(0.0043),\n  tensor(0.0007),\n  tensor(0.0027),\n  tensor(0.0026),\n  tensor(0.0053),\n  tensor(0.0194),\n  tensor(0.0080),\n  tensor(0.0207),\n  tensor(0.0002),\n  tensor(0.0081),\n  tensor(0.0172),\n  tensor(0.0032),\n  tensor(0.0045),\n  tensor(0.0009),\n  tensor(0.0185),\n  tensor(0.0075),\n  tensor(0.0183),\n  tensor(0.0506),\n  tensor(0.0043),\n  tensor(0.0011),\n  tensor(0.0011),\n  tensor(0.0022),\n  tensor(0.0035),\n  tensor(0.0011),\n  tensor(0.0034),\n  tensor(0.0005),\n  tensor(0.0009),\n  tensor(0.0008),\n  tensor(0.0011),\n  tensor(0.0141),\n  tensor(0.0010),\n  tensor(0.0027),\n  tensor(0.0007),\n  tensor(0.0007),\n  tensor(0.0128),\n  tensor(0.0125),\n  tensor(0.0120),\n  tensor(0.0003),\n  tensor(0.0002),\n  tensor(0.0005),\n  tensor(0.0004),\n  tensor(0.0047),\n  tensor(9.3667e-05),\n  tensor(0.0003),\n  tensor(0.0005),\n  tensor(0.0043),\n  tensor(5.6430e-05),\n  tensor(0.0003),\n  tensor(2.0903e-05),\n  tensor(0.0062),\n  tensor(2.0817e-05),\n  tensor(0.0002),\n  tensor(0.0021),\n  tensor(0.0004),\n  tensor(0.0005),\n  tensor(5.7401e-06),\n  tensor(0.0003),\n  tensor(3.2504e-05),\n  tensor(0.0003),\n  tensor(3.0881e-05),\n  tensor(0.0005),\n  tensor(0.0013),\n  tensor(0.0005),\n  tensor(0.0033),\n  tensor(0.0026),\n  tensor(7.0261e-05),\n  tensor(2.3055e-05),\n  tensor(0.0031),\n  tensor(0.0002),\n  tensor(0.0003),\n  tensor(0.0090),\n  tensor(0.0002),\n  tensor(0.0004),\n  tensor(8.5868e-05),\n  tensor(0.0029),\n  tensor(1.8972e-05),\n  tensor(0.0002),\n  tensor(2.5241e-05),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(1.0892e-05),\n  tensor(0.0003),\n  tensor(0.0015),\n  tensor(0.0002),\n  tensor(2.5078e-05),\n  tensor(5.1451e-05),\n  tensor(0.0002),\n  tensor(2.6138e-05),\n  tensor(0.0002),\n  tensor(7.0933e-05),\n  tensor(0.0011),\n  tensor(0.0005),\n  tensor(0.0001),\n  tensor(0.0013),\n  tensor(0.0002),\n  tensor(0.0003),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(0.0001),\n  tensor(0.0013),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(0.0013),\n  tensor(0.0010),\n  tensor(0.0002),\n  tensor(8.7324e-05),\n  tensor(7.0963e-05),\n  tensor(0.0002),\n  tensor(3.0527e-06),\n  tensor(0.0002),\n  tensor(0.0012),\n  tensor(0.0001),\n  tensor(0.0009),\n  tensor(3.2730e-05),\n  tensor(0.0002),\n  tensor(0.0007),\n  tensor(2.5720e-05),\n  tensor(8.5986e-06),\n  tensor(0.0011),\n  tensor(2.2715e-05),\n  tensor(0.0009),\n  tensor(0.0002),\n  tensor(6.3858e-05),\n  tensor(2.0692e-05),\n  tensor(3.3517e-05),\n  tensor(0.0002),\n  tensor(0.0011),\n  tensor(1.2475e-06),\n  tensor(6.0376e-05),\n  tensor(5.2503e-06),\n  tensor(6.1587e-05),\n  tensor(0.0008),\n  tensor(9.0122),\n  tensor(6.4943e-05),\n  tensor(0.0008),\n  tensor(7.2379),\n  tensor(0.0023),\n  tensor(6.0913e-05),\n  tensor(0.0134),\n  tensor(0.0182),\n  tensor(0.0065),\n  tensor(0.0672),\n  tensor(0.0357),\n  tensor(0.0043),\n  tensor(0.0138),\n  tensor(0.0333),\n  tensor(0.7574),\n  tensor(0.0859),\n  tensor(0.0102),\n  tensor(0.0135),\n  tensor(0.0059),\n  tensor(0.0974),\n  tensor(0.0051),\n  tensor(0.0257),\n  tensor(0.3059),\n  tensor(0.0017),\n  tensor(0.0012),\n  tensor(0.0036),\n  tensor(0.0027),\n  tensor(0.0368),\n  tensor(0.0006),\n  tensor(0.0008),\n  tensor(0.0007),\n  tensor(9.1583e-05),\n  tensor(0.0017),\n  tensor(0.0002),\n  tensor(0.0001),\n  tensor(0.0001),\n  tensor(0.0003),\n  tensor(9.2027e-05),\n  tensor(0.0003),\n  tensor(0.0002),\n  tensor(0.0034),\n  tensor(0.0016),\n  tensor(0.0002),\n  tensor(0.0025),\n  tensor(7.8547),\n  tensor(9.5973e-05),\n  tensor(0.0021),\n  tensor(0.0003),\n  tensor(0.0019),\n  tensor(0.0007),\n  tensor(0.0024),\n  tensor(0.0239),\n  tensor(0.0278),\n  tensor(0.0084),\n  tensor(0.0030),\n  tensor(0.0016),\n  tensor(0.0731),\n  tensor(0.0477),\n  tensor(0.0560),\n  tensor(0.0099),\n  tensor(0.0115),\n  tensor(0.0388),\n  tensor(0.0116),\n  tensor(0.0056),\n  tensor(5.7257),\n  tensor(0.0127),\n  tensor(0.8256),\n  tensor(0.0785),\n  tensor(0.0331),\n  tensor(0.1830),\n  tensor(0.3074),\n  tensor(0.3431),\n  tensor(0.1465),\n  tensor(0.0660),\n  tensor(0.0509),\n  tensor(0.1560),\n  tensor(0.0136),\n  tensor(0.0537),\n  tensor(0.0196),\n  tensor(0.0335),\n  tensor(0.0477),\n  tensor(0.0056),\n  tensor(0.0413),\n  tensor(0.0032),\n  tensor(0.0018),\n  tensor(0.0037),\n  tensor(0.0045),\n  tensor(0.1578),\n  tensor(0.0022),\n  tensor(0.6159),\n  tensor(0.0042),\n  tensor(0.0008),\n  tensor(0.0064),\n  tensor(0.0060),\n  tensor(0.0002),\n  tensor(0.0562),\n  tensor(8.9312e-05),\n  tensor(0.0006),\n  tensor(0.0038),\n  tensor(0.0010),\n  tensor(0.0018),\n  tensor(0.0009),\n  tensor(0.0005),\n  tensor(0.0031),\n  tensor(0.0020),\n  tensor(0.0019),\n  tensor(0.6533),\n  tensor(0.0004),\n  tensor(0.0004),\n  tensor(0.0019),\n  tensor(0.0507),\n  tensor(8.4712e-05),\n  tensor(0.0018),\n  tensor(0.0025),\n  tensor(0.0001),\n  tensor(0.0010),\n  tensor(0.0048),\n  tensor(0.0017),\n  tensor(0.0009),\n  tensor(0.0024),\n  tensor(0.0005),\n  tensor(0.0004),\n  tensor(0.0004),\n  tensor(0.0008),\n  tensor(0.0008),\n  tensor(0.0004),\n  tensor(0.1114),\n  tensor(0.0012),\n  tensor(0.0004),\n  tensor(0.0004),\n  tensor(0.0078),\n  tensor(0.0003),\n  tensor(0.0012),\n  tensor(0.0077),\n  tensor(0.0002),\n  tensor(0.0003),\n  tensor(0.0001),\n  tensor(0.0007),\n  tensor(4.1889e-05),\n  tensor(8.3576e-05),\n  tensor(0.0002),\n  tensor(0.0004),\n  tensor(0.0003),\n  tensor(0.0027),\n  tensor(0.0014),\n  tensor(0.0003),\n  tensor(0.0086),\n  tensor(4.5748e-05),\n  tensor(0.0005),\n  tensor(0.0016),\n  tensor(0.0003),\n  tensor(1.0663e-05),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(0.0045),\n  tensor(9.1546e-05),\n  tensor(0.0003),\n  tensor(0.0002),\n  tensor(0.0005),\n  tensor(0.0005),\n  tensor(0.0047),\n  tensor(0.0002),\n  tensor(0.0007),\n  tensor(0.0008),\n  tensor(0.0005),\n  tensor(0.0018),\n  tensor(0.0003),\n  tensor(0.0002),\n  tensor(7.8129e-06),\n  tensor(0.0028),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(8.7307e-06),\n  tensor(1.5375e-05),\n  tensor(5.1723e-05),\n  tensor(0.0011),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(0.0046),\n  tensor(0.0007),\n  tensor(5.9376e-05),\n  tensor(0.0002),\n  tensor(0.0023),\n  tensor(0.0003),\n  tensor(0.0065),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(0.0004),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(0.6067),\n  tensor(0.0004),\n  tensor(7.3969e-05),\n  tensor(0.0002),\n  tensor(0.6184),\n  tensor(0.0003),\n  tensor(0.0008),\n  tensor(0.0002),\n  tensor(0.0017),\n  tensor(0.0001),\n  tensor(0.0005),\n  tensor(0.0020),\n  tensor(0.0003),\n  tensor(0.0002),\n  tensor(0.0004),\n  tensor(0.0002),\n  tensor(0.0006),\n  tensor(0.0004),\n  tensor(0.0006),\n  tensor(0.0007),\n  tensor(0.0003),\n  tensor(0.0003),\n  tensor(3.6242),\n  tensor(0.0010),\n  tensor(0.0022),\n  tensor(0.0007),\n  tensor(0.0051),\n  tensor(0.0070),\n  tensor(0.0039),\n  tensor(0.0089),\n  tensor(0.0054),\n  tensor(0.0169),\n  tensor(0.0060),\n  tensor(0.0028),\n  tensor(0.0048),\n  tensor(0.0285),\n  tensor(0.0087),\n  tensor(0.0023),\n  tensor(0.0745),\n  tensor(0.0048),\n  tensor(0.0067),\n  tensor(0.0043),\n  tensor(0.0105),\n  tensor(0.0023),\n  tensor(0.0367),\n  tensor(0.0029),\n  tensor(0.0337),\n  tensor(0.0385),\n  tensor(0.0550),\n  tensor(0.0288),\n  tensor(0.0072),\n  tensor(0.0199),\n  tensor(0.0174),\n  tensor(0.0035),\n  tensor(0.0059),\n  tensor(0.0003),\n  tensor(0.0004),\n  tensor(1.6609),\n  tensor(0.0202),\n  tensor(0.0061),\n  tensor(0.0002),\n  tensor(0.0023),\n  tensor(0.0318),\n  tensor(0.0184),\n  tensor(0.1092),\n  tensor(0.0012),\n  tensor(0.0010),\n  tensor(0.0043),\n  tensor(0.0250),\n  tensor(7.3545e-05),\n  tensor(0.0008),\n  tensor(0.0548),\n  tensor(0.0025),\n  tensor(0.0032),\n  tensor(0.0120),\n  tensor(0.0010),\n  tensor(0.0011),\n  tensor(0.0015),\n  tensor(8.8139e-05),\n  tensor(0.0014),\n  tensor(0.0065),\n  tensor(0.0863),\n  tensor(3.4093e-05),\n  tensor(0.0010),\n  tensor(2.6558e-05),\n  tensor(0.0003),\n  tensor(0.0003),\n  tensor(6.6980),\n  tensor(0.0003),\n  tensor(0.0407),\n  tensor(0.0041),\n  tensor(0.0100),\n  tensor(0.7597),\n  tensor(0.0795),\n  tensor(0.2976),\n  tensor(0.0078),\n  tensor(0.0352),\n  tensor(0.0400),\n  tensor(0.0225),\n  tensor(0.0137),\n  tensor(0.0045),\n  tensor(0.0379),\n  tensor(0.0378),\n  tensor(0.0241),\n  tensor(0.2159),\n  tensor(0.0078),\n  tensor(0.0025),\n  tensor(0.0011),\n  tensor(0.0006),\n  tensor(0.0831),\n  tensor(0.0081),\n  tensor(0.0535),\n  tensor(0.0010),\n  tensor(0.0002),\n  tensor(0.0091),\n  tensor(0.0005),\n  tensor(0.3598),\n  tensor(5.8922e-05),\n  tensor(2.5588e-05),\n  tensor(0.0096),\n  tensor(0.1399),\n  tensor(9.2973e-06),\n  tensor(0.0012),\n  tensor(7.3880e-06),\n  tensor(1.1954e-05),\n  tensor(0.0014),\n  tensor(0.0005),\n  tensor(0.4765),\n  tensor(0.0277),\n  tensor(0.0012),\n  tensor(0.0001),\n  tensor(7.4051e-07),\n  tensor(2.3574e-06),\n  tensor(1.1034e-05),\n  tensor(4.7684e-07),\n  tensor(1.0208e-05),\n  tensor(1.9756e-06),\n  tensor(0.0012),\n  tensor(5.2472e-05),\n  tensor(0.0036),\n  tensor(0.0006),\n  tensor(0.0018),\n  tensor(4.9354e-06),\n  tensor(0.7763),\n  tensor(1.9136e-06),\n  tensor(0.0013),\n  tensor(0.0013),\n  tensor(0.0013),\n  tensor(2.4869e-05),\n  tensor(7.1194e-05),\n  tensor(9.8246e-06),\n  tensor(0.0011),\n  tensor(1.4242e-07),\n  tensor(9.7509e-06),\n  tensor(2.3087e-05),\n  tensor(0.0005),\n  tensor(0.0011),\n  tensor(0.0011),\n  tensor(0.0099),\n  tensor(6.5168e-06),\n  tensor(2.4886e-05),\n  tensor(4.0076e-07),\n  tensor(1.4921e-06),\n  tensor(0.0011),\n  tensor(9.6725e-07),\n  tensor(0.0011),\n  tensor(1.1921e-07),\n  tensor(1.2756e-06),\n  tensor(0.0011),\n  tensor(0.0098),\n  tensor(0.0001),\n  tensor(0.0100),\n  tensor(9.0476e-07),\n  tensor(5.5889e-07),\n  tensor(5.9532e-07),\n  tensor(0.0010),\n  tensor(0.0001),\n  tensor(5.3518e-05),\n  tensor(2.4769e-05),\n  tensor(1.1117e-06),\n  tensor(2.2787e-05),\n  tensor(0.0369),\n  tensor(1.0715e-06),\n  tensor(0.0008),\n  tensor(2.8072e-06),\n  tensor(0.0083),\n  tensor(1.3492e-07),\n  tensor(1.0102e-05),\n  tensor(2.5156e-07),\n  tensor(0.0008),\n  tensor(1.5902e-07),\n  tensor(5.2594e-07),\n  tensor(0.0002),\n  tensor(6.2784e-05),\n  tensor(4.6366e-07),\n  tensor(0.8840),\n  tensor(4.8489e-07),\n  tensor(2.3442e-05),\n  tensor(9.0402e-05),\n  tensor(7.7679e-06),\n  tensor(0.0010),\n  tensor(2.5090e-05),\n  tensor(0.0010),\n  tensor(0.0011),\n  tensor(4.3898),\n  tensor(3.5771e-06),\n  tensor(0.0142),\n  tensor(0.0072),\n  tensor(5.9295e-05),\n  tensor(0.0024),\n  tensor(0.0005),\n  tensor(0.0239),\n  tensor(0.0041),\n  tensor(0.0063),\n  tensor(0.0398),\n  tensor(0.0045),\n  tensor(0.0088),\n  tensor(0.0008),\n  tensor(0.0501),\n  tensor(0.0090),\n  tensor(0.0499),\n  tensor(0.0463),\n  tensor(8.0043e-05),\n  tensor(0.0094),\n  tensor(0.0350),\n  tensor(0.0009),\n  tensor(0.0301),\n  tensor(0.1333),\n  tensor(0.0003),\n  tensor(0.0201),\n  tensor(0.0172),\n  tensor(0.0004),\n  tensor(0.1007),\n  tensor(7.0192e-05),\n  tensor(0.0076),\n  tensor(0.0005),\n  tensor(0.0003),\n  tensor(0.0268),\n  tensor(6.7038e-05),\n  tensor(0.0064),\n  tensor(2.7497e-05),\n  tensor(0.0019),\n  tensor(0.0051),\n  tensor(0.0122),\n  tensor(2.2152),\n  tensor(0.0038),\n  tensor(0.0004),\n  tensor(0.0003),\n  tensor(2.6469e-06),\n  tensor(0.0020),\n  tensor(0.0018),\n  tensor(0.0002),\n  tensor(9.8521e-07),\n  tensor(15.9891),\n  tensor(0.0002),\n  tensor(3.7967e-05),\n  tensor(2.3698e-05),\n  tensor(0.0002),\n  tensor(0.0109),\n  tensor(0.0146),\n  tensor(0.0005),\n  tensor(0.0047),\n  tensor(0.0275),\n  tensor(0.0153),\n  tensor(0.5198),\n  tensor(0.0142),\n  tensor(0.0973),\n  tensor(0.0020),\n  tensor(0.0014),\n  tensor(0.0182),\n  tensor(0.0132),\n  tensor(0.0217),\n  tensor(0.0675),\n  tensor(0.0203),\n  tensor(0.0092),\n  tensor(0.0273),\n  tensor(0.0051),\n  tensor(0.0242),\n  tensor(0.0179),\n  tensor(0.0092),\n  tensor(0.0125),\n  tensor(0.0064),\n  tensor(0.0024),\n  tensor(0.0002),\n  tensor(0.0042),\n  tensor(0.0063),\n  tensor(0.0014),\n  tensor(0.0005),\n  tensor(0.0721),\n  tensor(0.0002),\n  tensor(0.0016),\n  tensor(0.0032),\n  tensor(0.0025),\n  tensor(0.0001),\n  tensor(4.3579e-05),\n  tensor(0.0018),\n  tensor(0.0031),\n  tensor(0.0012),\n  tensor(0.0010),\n  tensor(0.0012),\n  tensor(0.0029),\n  tensor(0.0011),\n  tensor(0.0027),\n  tensor(0.0005),\n  tensor(0.0010),\n  tensor(0.0631),\n  tensor(0.0018),\n  tensor(0.0319),\n  tensor(3.9465e-05),\n  tensor(0.0043),\n  tensor(0.0024),\n  tensor(0.0020),\n  tensor(0.0006),\n  tensor(6.8799e-05),\n  tensor(0.0006),\n  tensor(6.1770e-05),\n  tensor(2.7602),\n  tensor(0.0017),\n  tensor(0.0007),\n  tensor(0.0033),\n  tensor(0.0004),\n  tensor(0.0060),\n  tensor(0.0016),\n  tensor(0.0035),\n  tensor(0.0016),\n  tensor(6.1332),\n  tensor(0.0024),\n  tensor(0.0090),\n  tensor(0.0184),\n  tensor(0.0088),\n  tensor(0.0035),\n  tensor(0.0938),\n  tensor(0.0884),\n  tensor(0.1101),\n  tensor(0.1089),\n  tensor(0.2253),\n  tensor(0.0612),\n  tensor(0.3631),\n  tensor(0.0924),\n  tensor(1.6158),\n  tensor(1.0882),\n  tensor(0.0178),\n  tensor(0.0183),\n  tensor(0.0007),\n  tensor(0.0004),\n  tensor(0.0055),\n  tensor(0.0001),\n  tensor(2.6178e-05),\n  tensor(4.6236e-05),\n  tensor(0.0013),\n  tensor(3.4125e-05),\n  tensor(0.0008),\n  tensor(0.0055),\n  tensor(6.1528e-06),\n  tensor(0.0031),\n  tensor(0.0001),\n  tensor(4.5435e-07),\n  tensor(0.0001),\n  tensor(6.1519e-05),\n  tensor(7.5205e-07),\n  tensor(1.7903e-06),\n  tensor(2.9105e-06),\n  tensor(5.9335),\n  tensor(3.2189e-05),\n  tensor(0.0002),\n  tensor(0.0009),\n  tensor(0.0003),\n  tensor(1.4081e-06),\n  tensor(7.1428e-06),\n  tensor(0.0020),\n  tensor(0.0002),\n  tensor(0.0008),\n  tensor(3.3939e-05),\n  tensor(0.0003),\n  tensor(0.0010),\n  tensor(0.0006),\n  tensor(0.0016),\n  tensor(6.8542e-05),\n  tensor(0.0022),\n  tensor(5.4975e-05),\n  tensor(0.0055),\n  tensor(5.7608e-05),\n  tensor(2.1068e-05),\n  tensor(0.0014),\n  tensor(0.0001),\n  tensor(7.0774e-06),\n  tensor(0.0003),\n  tensor(0.0036),\n  tensor(0.0003),\n  tensor(0.0005),\n  tensor(0.0055),\n  tensor(0.0010),\n  tensor(0.0060),\n  tensor(0.0030),\n  tensor(0.0059),\n  tensor(0.0057),\n  tensor(0.0018),\n  tensor(7.8009e-05),\n  tensor(0.0017),\n  tensor(0.0009),\n  tensor(0.0008),\n  tensor(6.6469e-05),\n  tensor(0.0021),\n  tensor(5.5737e-05),\n  tensor(0.0006),\n  tensor(0.0041),\n  tensor(0.0054),\n  tensor(0.0017),\n  tensor(0.0026),\n  tensor(6.3265e-05),\n  tensor(0.0044),\n  tensor(0.0016),\n  tensor(0.0001),\n  tensor(0.0203),\n  tensor(0.0034),\n  tensor(0.0019),\n  tensor(0.0013),\n  tensor(6.5594e-05),\n  tensor(0.0063),\n  tensor(0.0048),\n  tensor(0.0083),\n  tensor(6.6588e-06),\n  tensor(0.0048),\n  tensor(0.0046),\n  tensor(0.0037),\n  tensor(9.1066e-05),\n  tensor(0.0043),\n  tensor(4.1942),\n  tensor(0.0016),\n  tensor(0.1798),\n  tensor(6.9499e-05),\n  tensor(0.0527),\n  tensor(0.0047),\n  tensor(3.1806e-05),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(0.0059),\n  tensor(0.0088),\n  tensor(0.0090),\n  tensor(0.0069),\n  tensor(0.0004),\n  tensor(0.0002),\n  tensor(0.0032),\n  tensor(0.0089),\n  tensor(0.0004),\n  tensor(0.0285),\n  tensor(0.0001),\n  tensor(4.1754e-05),\n  tensor(1.7240e-05),\n  tensor(3.4893e-05),\n  tensor(0.0002),\n  tensor(0.0041),\n  tensor(0.1239),\n  tensor(0.0075),\n  tensor(0.0002),\n  tensor(0.0060),\n  tensor(0.0031),\n  tensor(2.2428e-05),\n  tensor(0.0001),\n  tensor(1.7526e-05),\n  tensor(0.0036),\n  tensor(0.1533),\n  tensor(0.0064),\n  tensor(1.4952e-05),\n  tensor(0.0001),\n  tensor(0.0161),\n  tensor(5.5238e-05),\n  tensor(0.0033),\n  tensor(0.0029),\n  tensor(1.9966e-06),\n  tensor(9.4949e-06),\n  tensor(0.0049),\n  tensor(0.0013),\n  tensor(0.0053),\n  tensor(0.0023),\n  tensor(0.0006),\n  tensor(3.8015e-05),\n  tensor(0.0024),\n  tensor(5.6981e-06),\n  tensor(2.4040e-05),\n  tensor(0.0020),\n  tensor(8.2982e-06),\n  tensor(3.8464e-05),\n  tensor(2.9717e-05),\n  tensor(0.0252),\n  tensor(10.8600),\n  tensor(8.3773e-05),\n  tensor(0.0066),\n  tensor(0.0005),\n  tensor(1.3223),\n  tensor(0.0137),\n  tensor(0.0166),\n  tensor(0.0414),\n  tensor(0.0794),\n  tensor(0.0011),\n  tensor(0.0041),\n  tensor(0.0089),\n  tensor(0.0303),\n  tensor(0.0156),\n  tensor(0.0135),\n  tensor(0.0138),\n  tensor(0.0335),\n  tensor(0.0039),\n  tensor(0.0038),\n  tensor(0.5820),\n  tensor(0.0174),\n  tensor(0.0014),\n  tensor(0.0018),\n  tensor(0.0445),\n  tensor(0.0026),\n  tensor(0.0599),\n  tensor(0.0035),\n  tensor(0.0244),\n  tensor(0.0028),\n  tensor(0.0002),\n  tensor(0.0036),\n  tensor(0.0172),\n  tensor(0.0007),\n  tensor(0.0055),\n  tensor(0.0001),\n  tensor(0.0040),\n  tensor(0.0010),\n  tensor(0.0024),\n  tensor(0.0036),\n  tensor(0.0007),\n  tensor(0.0007),\n  tensor(0.9362),\n  tensor(0.0016),\n  tensor(0.0001),\n  tensor(0.0008),\n  tensor(0.0034),\n  tensor(9.0371e-05),\n  tensor(8.4657e-05),\n  tensor(0.0003),\n  tensor(1.6980e-05),\n  tensor(2.1044e-05),\n  tensor(0.0198),\n  tensor(6.2970e-07),\n  tensor(0.0002),\n  tensor(2.3573e-05),\n  tensor(2.6527e-05),\n  tensor(8.7960e-06),\n  tensor(7.9466e-05),\n  tensor(4.9912e-05),\n  tensor(2.1138e-06),\n  tensor(0.0011),\n  tensor(3.6820e-06),\n  tensor(0.0010),\n  tensor(8.8503e-06),\n  tensor(9.0631e-05),\n  tensor(3.5899e-06),\n  tensor(2.4357e-05),\n  tensor(8.9847e-05),\n  tensor(9.3904e-06),\n  tensor(9.3633e-05),\n  tensor(0.0009),\n  tensor(4.5402e-07),\n  tensor(1.2583e-05),\n  tensor(8.2560e-06),\n  tensor(1.7886e-05),\n  tensor(7.7822e-06),\n  tensor(7.6553e-05),\n  tensor(0.0001),\n  tensor(7.5630e-06),\n  tensor(0.0001),\n  tensor(4.8472e-07),\n  tensor(6.7269e-05),\n  tensor(3.8135e-05),\n  tensor(9.2819e-07),\n  tensor(4.6907e-07),\n  tensor(8.6157e-06),\n  tensor(6.7797e-05),\n  tensor(2.1604e-05),\n  tensor(3.8118e-05),\n  tensor(3.7604e-05),\n  tensor(6.9086e-06),\n  tensor(6.8974e-06),\n  tensor(1.2299e-06),\n  tensor(6.8573e-06),\n  tensor(0.6054),\n  tensor(0.0058),\n  tensor(6.7276e-07),\n  tensor(5.1103e-06),\n  tensor(6.1864e-05),\n  tensor(0.0002),\n  tensor(2.1743e-06),\n  tensor(1.0549e-05),\n  tensor(6.6385e-05),\n  tensor(0.0037),\n  tensor(0.0012),\n  tensor(1.3583e-05),\n  tensor(0.0010),\n  tensor(1.4443e-05),\n  tensor(0.0002),\n  tensor(3.3069e-06),\n  tensor(0.0001),\n  tensor(0.0002),\n  tensor(0.0002),\n  tensor(0.0010),\n  tensor(1.4758e-05),\n  tensor(1.7314e-05),\n  tensor(6.9949e-05),\n  tensor(1.4256e-05),\n  tensor(7.1554e-05),\n  tensor(2.3024e-05),\n  tensor(0.0002),\n  tensor(0.0001),\n  tensor(0.0012),\n  tensor(8.7468e-07),\n  tensor(8.0807e-05),\n  tensor(2.5187e-06),\n  tensor(7.4586e-05),\n  ...]}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "X_tensors = [torch.tensor(seq, dtype=torch.long) for seq in X]\n",
    "X_padded = pad_sequence(X_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "Y_tensors = [torch.tensor(seq, dtype=torch.long) for seq in Y]\n",
    "Y_padded = pad_sequence(Y_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "attention_masks_tensor = [torch.tensor(seq, dtype=torch.long) for seq in attention_masks]\n",
    "attention_masks_padded = pad_sequence(attention_masks_tensor, batch_first=True, padding_value=0)\n",
    "\n",
    "train_model(model, X_padded, Y_padded, attention_masks_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a6933",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "Créer une fonction prenant les prédictions du modèle (au niveau token) et sort les prédictions au niveau mot.<br>\n",
    "Par exemple, admettons que, pour un mot, la prédiction du 1er token est la seule qu'on considère.<br>\n",
    "si la phrase est \"Bonjour John\", avec les tokens \\[\"bon\", \"jour\", \"Jo\", \"hn\"\\] avec les predictions \\[0.12, 0.65, 0.88, 0.45\\]<br>\n",
    "Je veux récupérer les prédictions \"bonjour\": 0.12, \"John\": 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a0316b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:59:42.390793Z",
     "start_time": "2023-11-28T21:59:42.261245500Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjourJohn': 0.12}\n"
     ]
    }
   ],
   "source": [
    "def word_level_predictions(tokens, token_predictions):\n",
    "    word_predictions = {}\n",
    "    current_word = \"\"\n",
    "    current_prediction = 0.0\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Si le token est le début d'un nouveau mot ou la continuation d'un mot existant\n",
    "        if i == 0 or not tokens[i-1].isalpha() or not token.isalpha():\n",
    "            if current_word:  # S'il y a un mot précédent, l'ajouter aux prédictions\n",
    "                word_predictions[current_word] = current_prediction\n",
    "            current_word = token\n",
    "            current_prediction = token_predictions[i]\n",
    "        else:  # Si le token est une continuation sans préfixe\n",
    "            current_word += token\n",
    "\n",
    "    # Ajouter la prédiction du dernier mot\n",
    "    if current_word:\n",
    "        word_predictions[current_word] = current_prediction\n",
    "\n",
    "    return word_predictions\n",
    "\n",
    "# Exemple d'utilisation\n",
    "tokens = [\"bon\", \"jour\", \"Jo\", \"hn\"]\n",
    "token_predictions = [0.12, 0.65, 0.88, 0.45]\n",
    "word_predictions = word_level_predictions(tokens, token_predictions)\n",
    "print(word_predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T23:33:56.995621600Z",
     "start_time": "2023-11-19T23:33:56.985296500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8562e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
